# Local LLM

Running LLM's  on your local machine using Ollama with an integrated web-ui.


## Running it locally

```shell
docker compose up -d
```

The web-ui will be available at: 
> http://localhost:3000


## Relevant links

- [Ollama models](https://ollama.com/library)
- [Web UI Repo](https://github.com/open-webui/open-webui)
